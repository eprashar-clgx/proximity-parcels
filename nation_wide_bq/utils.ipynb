{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bc00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "from google.cloud import storage\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery_datatransfer\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e2323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to check authentication key\n",
    "# Function to check google authentication token and re-generate if it is expired/doesn't exist\n",
    "def check_and_authenticate(json_path):\n",
    "    '''\n",
    "    Function to check google authentication token and re-generate if it is expired/doesn't exist\n",
    "    '''\n",
    "    try:\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"Credentials file not found\")\n",
    "        # Get modification time of the file\n",
    "        file_mod_time = datetime.fromtimestamp(os.path.getmtime(json_path))\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Check if the file is older than 24 hours\n",
    "        if current_time - file_mod_time > timedelta(hours=24):\n",
    "            print(\"Credentials file is older than 24 hours. Re-authenticating...\")\n",
    "\n",
    "            # Re-authenticate\n",
    "            try:\n",
    "                print(f\"Trying reauthentication on gcloud server using shell command...\")\n",
    "                subprocess.run(\"start cmd /c gcloud auth application-default login\", shell=True, check=True)\n",
    "                print('Login window opened...please complete authentication')\n",
    "                \n",
    "                # Poll for file modification\n",
    "                print(\"Waiting for credentials file to update...\")\n",
    "                max_wait = 300  # seconds\n",
    "                check_interval = 2  # seconds\n",
    "                start_time = datetime.now()\n",
    "\n",
    "                while (datetime.now() - start_time).total_seconds() < max_wait:\n",
    "                    new_mod_time = datetime.fromtimestamp(os.path.getmtime(json_path))\n",
    "                    if new_mod_time > file_mod_time:\n",
    "                        print(\"Authentication confirmed! Credentials file updated.\")\n",
    "                        break\n",
    "                    time.sleep(check_interval)\n",
    "                else:\n",
    "                    print(\"Timed out waiting for credentials file update.\")\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error during re-authentication: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f'Authentication failed because of {e}')\n",
    "        else:\n",
    "            print(\"Credentials file is valid.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72d9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is older than 24 hours. Re-authenticating...\n",
      "Trying reauthentication on gcloud server using shell command...\n",
      "Login window opened...please complete authentication\n",
      "Waiting for credentials file to update...\n",
      "Authentication confirmed! Credentials file updated.\n"
     ]
    }
   ],
   "source": [
    "# Uploading to GCS\n",
    "# First, validate the authentication token\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Check and authenticate\n",
    "check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfcd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not using a service account, initialize the client like this:\n",
    "SOURCE_PROJECT_ID = \"clgx-gis-app-uat-a0e0\"\n",
    "SOURCE_DATASET_ID = \"proximity_parcels\"\n",
    "\n",
    "DESTINATION_PROJECT_ID = \"clgx-gis-app-prd-364d\"\n",
    "DESTINATION_DATASET_ID = \"proximity_parcels\"\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID)\n",
    "\n",
    "# ==============================================================================\n",
    "# Function to copy all tables from source dataset to destination dataset\n",
    "# ==============================================================================\n",
    "def copy_all_tables(\n",
    "        source_project, \n",
    "        source_dataset, \n",
    "        dest_project, \n",
    "        dest_dataset,\n",
    "        tables=None,\n",
    "        overwrite=False\n",
    "        ):\n",
    "    \"\"\"\n",
    "   Copies tables from a source dataset to a destination dataset.\n",
    "    - If 'table_list' is provided, it copies only those tables.\n",
    "    - If 'table_list' is None, it copies all tables from the source.\n",
    "    - It will automatically skip any materialized views.\n",
    "    - If 'overwrite' is False, it will skip tables that already exist in the destination.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Table Copy ---\")\n",
    "    print(f\"From: {source_project}.{source_dataset}\")\n",
    "    print(f\"To:   {dest_project}.{dest_dataset}\")\n",
    "\n",
    "    source_dataset_ref = f\"{source_project}.{source_dataset}\"\n",
    "    \n",
    "    try:\n",
    "        # Determine which tables to process\n",
    "        tables_to_process = []\n",
    "        if tables:\n",
    "            print(f\"Processing a provided list of {len(tables)} tables.\")\n",
    "            tables_to_process = tables\n",
    "        else:\n",
    "            print(\"No table list provided. Fetching all tables from source dataset.\")\n",
    "            all_items = client.list_tables(source_dataset_ref)\n",
    "            tables_to_process = [item.table_id for item in all_items]\n",
    "            print(f\"Found {len(tables_to_process)} items in source dataset.\")\n",
    "\n",
    "        for table_id in tables_to_process:\n",
    "            source_table_ref_str = f\"{source_project}.{source_dataset}.{table_id}\"\n",
    "        \n",
    "            # Get the full table object to check its type\n",
    "            try:\n",
    "                table_obj = client.get_table(source_table_ref_str)\n",
    "            except NotFound:\n",
    "                print(f\"  -> WARNING: Table '{table_id}' not found in source dataset. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Check the table_type\n",
    "            if table_obj.table_type == \"MATERIALIZED_VIEW\":\n",
    "                print(f\"  -> Skipping: {table_id} (Type: MATERIALIZED_VIEW)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  -> Copying table: {table_id}...\")\n",
    "            dest_table_ref_str = f\"{dest_project}.{dest_dataset}.{table_id}\"\n",
    "\n",
    "            # Additional check for overwrite functionality\n",
    "            if not overwrite:\n",
    "                try:\n",
    "                    client.get_table(dest_table_ref_str)\n",
    "                    # If get_table succeeds, the table exists.\n",
    "                    print(f\"  -> Skipping: Destination table '{dest_table_ref_str}' already exists and overwrite is False.\")\n",
    "                    continue\n",
    "                except NotFound:\n",
    "                    # Table doesn't exist, so we can proceed with the copy.\n",
    "                    pass\n",
    "\n",
    "            # Configure and start the copy job\n",
    "            job_config = bigquery.CopyJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "            copy_job = client.copy_table(\n",
    "                source_table_ref_str,\n",
    "                dest_table_ref_str,\n",
    "                job_config=job_config,\n",
    "            )\n",
    "            copy_job.result()  # Wait for the job to complete\n",
    "            print(f\"      -> SUCCESS: Copied to {dest_table_ref_str}\")\n",
    "\n",
    "    except NotFound:\n",
    "        print(f\"ERROR: Source dataset '{source_dataset_ref}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during table copy: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56a8ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Table Copy ---\n",
      "From: clgx-gis-app-uat-a0e0.proximity_parcels\n",
      "To:   clgx-gis-app-prd-364d.proximity_parcels\n",
      "Processing a provided list of 5 tables.\n",
      "  -> Copying table: roadways...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.roadways' already exists and overwrite is False.\n",
      "  -> Copying table: railways...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.railways' already exists and overwrite is False.\n",
      "  -> Copying table: transmission_lines...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.transmission_lines' already exists and overwrite is False.\n",
      "  -> Copying table: protected_lands_national...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.protected_lands_national' already exists and overwrite is False.\n",
      "  -> Copying table: wetlands...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.wetlands' already exists and overwrite is False.\n"
     ]
    }
   ],
   "source": [
    "# Copy tables from GIS UAT to PROD\n",
    "tables_to_copy = [\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'transmission_lines',\n",
    "    'protected_lands_national',\n",
    "    'wetlands'\n",
    "    ]\n",
    "\n",
    "# Run copy all tables\n",
    "copy_all_tables(\n",
    "    SOURCE_PROJECT_ID, \n",
    "    SOURCE_DATASET_ID, \n",
    "    DESTINATION_PROJECT_ID, \n",
    "    DESTINATION_DATASET_ID,\n",
    "    tables=tables_to_copy,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541642b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to copy procedures from .sql files\n",
    "def create_procedures_from_sql_files(\n",
    "        sql_folder_path, \n",
    "        project_id, \n",
    "        dataset_id\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Scans a directory for .sql files and creates (or replaces) them as\n",
    "    stored procedures in the specified BigQuery dataset.\n",
    "\n",
    "    Args:\n",
    "        sql_folder_path (str): The local path to the folder containing .sql files.\n",
    "        project_id (str): The target GCP project ID.\n",
    "        dataset_id (str): The target BigQuery dataset ID.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Procedure Creation from folder: {sql_folder_path} ---\")\n",
    "    \n",
    "    if not os.path.isdir(sql_folder_path):\n",
    "        print(f\"ERROR: Directory not found at '{sql_folder_path}'\")\n",
    "        return\n",
    "\n",
    "    sql_files = [f for f in os.listdir(sql_folder_path) if f.endswith('.sql')]\n",
    "    print(f\"Found {len(sql_files)} .sql files to process.\")\n",
    "\n",
    "    for sql_file in sql_files:\n",
    "        procedure_name = os.path.splitext(sql_file)[0]\n",
    "        full_file_path = os.path.join(sql_folder_path, sql_file)\n",
    "        \n",
    "        print(f\"  -> Processing: {sql_file} (Procedure: {procedure_name})\")\n",
    "\n",
    "        try:\n",
    "            with open(full_file_path, 'r') as f:\n",
    "                # The content of the file is the full DDL statement\n",
    "                sql_body = f.read()\n",
    "            \n",
    "            # The .sql file should contain the full CREATE OR REPLACE PROCEDURE statement.\n",
    "            # We just need to execute it.\n",
    "            query_job = client.query(sql_body)\n",
    "            query_job.result()  # Wait for the job to complete\n",
    "            print(f\"-> SUCCESS: Created or replaced procedure '{project_id}.{dataset_id}.{procedure_name}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"-> ERROR: Failed to create procedure from {sql_file}. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c06a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run procedures\n",
    "def execute_calls_from_file(calls_file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file line by line and executes each line as a BQ query.\n",
    "    Ideal for running a series of CALL statements.\n",
    "\n",
    "    Args:\n",
    "        calls_file_path (str): The local path to the .txt file with CALL statements.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Executing Calls from file: {calls_file_path} ---\")\n",
    "\n",
    "    if not os.path.exists(calls_file_path):\n",
    "        print(f\"ERROR: File not found at '{calls_file_path}'\")\n",
    "        return\n",
    "        \n",
    "    with open(calls_file_path, 'r') as f:\n",
    "        # Read all lines, filtering out empty ones\n",
    "        calls_to_make = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"Found {len(calls_to_make)} procedure calls to execute.\")\n",
    "\n",
    "    for i, call_statement in enumerate(calls_to_make):\n",
    "        print(f\"  -> Executing call {i+1}/{len(calls_to_make)}: {call_statement}\")\n",
    "        try:\n",
    "            query_job = client.query(call_statement)\n",
    "            query_job.result() # Wait for the call to complete\n",
    "            print(\"     -> SUCCESS: Call completed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"     -> ERROR: Call failed. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb390229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Executing Calls from file: C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\feb_25_encumbered_parcels\\enc-parcels\\nation_wide_bq\\run_procedures.txt ---\n",
      "Found 15 procedure calls to execute.\n",
      "  -> Executing call 1/15: -- Run procedures one by one\n",
      "     -> ERROR: Call failed. Reason: 400 Syntax error: Unexpected end of statement at [1:29]; reason: invalidQuery, location: query, message: Syntax error: Unexpected end of statement at [1:29]\n",
      "\n",
      "Location: US\n",
      "Job ID: 23b64d21-a990-4d4d-bb84-f211c36fb494\n",
      "\n",
      "  -> Executing call 2/15: CALL proximity_parcels.create_materialized_view('parcels');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 3/15: CALL proximity_parcels.create_materialized_view('roadways');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 4/15: CALL proximity_parcels.create_materialized_view('railways');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 5/15: CALL proximity_parcels.create_materialized_view('transmission_lines');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 6/15: CALL proximity_parcels.create_materialized_view('protected_lands_national');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 7/15: CALL proximity_parcels.create_materialized_view('wetlands');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 8/15: CALL proximity_parcels.calculate_proximity_score_lines_batch('roadways','ID');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 9/15: CALL proximity_parcels.calculate_proximity_score_lines_batch('railways', 'FRAARCID');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 10/15: CALL proximity_parcels.calculate_proximity_score_lines_batch('transmission_lines','ID');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 11/15: CALL proximity_parcels.calculate_proximity_score_polygons_batch('protected_lands_national','ID');\n",
      "     -> SUCCESS: Call completed.\n",
      "  -> Executing call 12/15: CALL proximity_parcels.calculate_proximity_score_polygons_batch('wetlands','NWI_ID');\n"
     ]
    }
   ],
   "source": [
    "# Run procedures in text file\n",
    "CALLS_FILE_PATH = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\feb_25_encumbered_parcels\\enc-parcels\\nation_wide_bq\\run_procedures.txt\"\n",
    "execute_calls_from_file(CALLS_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
